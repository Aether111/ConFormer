{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omteMWGLPVMp",
        "outputId": "d164e3eb-d011-453c-9724-ba8b1271bea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.0-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Collecting tensorflow>=2.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from keras-tuner) (2.28.1)\n",
            "Requirement already satisfied: ipython in /usr/lib/python3/dist-packages (from keras-tuner) (7.13.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (2.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (1.11.2)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (3.11.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (1.1.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (45.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.20 in ./.local/lib/python3.8/site-packages (from tensorflow>=2.0->keras-tuner) (1.23.4)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (1.29.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m167.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in ./.local/lib/python3.8/site-packages (from tensorflow>=2.0->keras-tuner) (4.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/lib/python3/dist-packages (from ipython->keras-tuner) (4.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging->keras-tuner) (2.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->keras-tuner) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->keras-tuner) (2019.11.28)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m209.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.34.2)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.2.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.1.1)\n",
            "Installing collected packages: tensorboard-plugin-wit, libclang, kt-legacy, flatbuffers, werkzeug, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, keras, google-auth, absl-py, tensorboard, tensorflow, keras-tuner\n",
            "Successfully installed absl-py-1.4.0 flatbuffers-23.3.3 google-auth-2.16.2 keras-2.11.0 keras-tuner-1.3.0 kt-legacy-1.0.4 libclang-15.0.6.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 werkzeug-2.2.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNt6i48pPVMt",
        "outputId": "99479527-c6ed-4385-91e6-488f2e075b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install protobuf==3.20.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo3PyWZVRqMK"
      },
      "outputs": [],
      "source": [
        "from skimage.draw import polygon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras_tuner as kt\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLfMI7RpVG-T",
        "outputId": "c93f7acc-2a91-4e0c-ac47-25537fda9ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/EchoNet-Dynamic/EchoNet-Dynamic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJCnKNZhVTl3",
        "outputId": "381f6ee3-7a55-4145-e3c3-ad96797edfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bI7fEoNXBdp_eeIGjz3uPx7LaVeQuwhJ/EchoNet-Dynamic/EchoNet-Dynamic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgk3gfc8mmLM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "image_size = (224,224,3)\n",
        "epochs = 10\n",
        "learning_rate = 1e-4\n",
        "path = r\"\"\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWYfuRtPqvnH"
      },
      "outputs": [],
      "source": [
        "metadata = pd.read_csv(os.path.join(path,\"FileList.csv\"))\n",
        "metadata.drop(axis=0,index=np.arange(10025,10030),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz2WHliDPVMw",
        "outputId": "ef8c4180-0205-4c6e-f98d-b1f76470708c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/Videos\n"
          ]
        }
      ],
      "source": [
        "cd Videos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xozQHcaCPVMx",
        "outputId": "990b0d4e-1b84-49d2-ade1-52a0d2c3f98e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10030"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glob.glob(\"*.avi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORBp0TlQPVMy",
        "outputId": "a3425ede-dc12-46f9-a07e-de4b351e55f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CFcofOjoGx5"
      },
      "outputs": [],
      "source": [
        "vols = pd.read_csv(\"VolumeTracings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "WI8hc6uPEPRI",
        "outputId": "0333b936-d7d5-475c-cccc-5a768243b086"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FileName</th>\n",
              "      <th>EF</th>\n",
              "      <th>ESV</th>\n",
              "      <th>EDV</th>\n",
              "      <th>FrameHeight</th>\n",
              "      <th>FrameWidth</th>\n",
              "      <th>FPS</th>\n",
              "      <th>NumberOfFrames</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [FileName, EF, ESV, EDV, FrameHeight, FrameWidth, FPS, NumberOfFrames, Split]\n",
              "Index: []"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vols[vols[\"FileName\"] == \"0X234005774F4CB5CD\"]\n",
        "metadata[metadata[\"FileName\"] == \"0X234005774F4CB5CD\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzhQMe0HUWL7"
      },
      "outputs": [],
      "source": [
        "def identity_block(input,kernel_size,filters):\n",
        "    filter1,filter2,filter3 = filters\n",
        "    bn_axis = 3\n",
        "    x = Conv2D(filters=filter1,kernel_size=(1,1),data_format=\"channels_last\")(input)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=filter2,kernel_size=kernel_size,padding=\"same\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=filter3,kernel_size=(1,1),data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.add([x,input])\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSVtaxiCW_8t"
      },
      "outputs": [],
      "source": [
        "def conv_block(input,kernel_size,filters,strides=(2,2)):\n",
        "    filter1,filter2,filter3 = filters\n",
        "    bn_axis = 3\n",
        "    x = Conv2D(filters=filter1,kernel_size=(1,1),strides=strides,data_format=\"channels_last\")(input)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=filter2,kernel_size=kernel_size,padding=\"same\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=filter3,kernel_size=(1,1),data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "\n",
        "    shortcut = Conv2D(filters=filter3,kernel_size=(1,1),strides=strides,data_format=\"channels_last\")(input)\n",
        "    shortcut = BatchNormalization(axis=bn_axis)(shortcut)\n",
        "    x = layers.add([x,shortcut])\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhIQz4zaZDVi"
      },
      "outputs": [],
      "source": [
        "def one_side_pad(x):\n",
        "    x = ZeroPadding2D((1,1),data_format=\"channels_last\")\n",
        "    x = Lambda(lambda x: x[:,:-1,:-1,:])(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMQ0Iy0vGTxf"
      },
      "outputs": [],
      "source": [
        "def ASPP(input,atrous_rates,out_channels):\n",
        "    res = []\n",
        "    dims = input.shape\n",
        "    x = AveragePooling2D((dims[-3],dims[-2]))(input)\n",
        "    x = Conv2D(kernel_size=(1,1),filters=out_channels,use_bias=True)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D((dims[-3]//x.shape[1],dims[-2]//x.shape[2]),interpolation=\"bilinear\")(x)\n",
        "    res.append(x)\n",
        "    for dilation in atrous_rates:\n",
        "        x = ZeroPadding2D(padding=(dilation,dilation))(x)\n",
        "        x = Conv2D(kernel_size=(3,3),filters=out_channels,dilation_rate=dilation,padding=\"valid\",use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        res.append(x)\n",
        "    size = x.shape[-3:-1]\n",
        "    #x = AveragePooling2D(pool_size=(3,3))(x)\n",
        "    #x = Lambda(lambda x: tf.image.resize(x,size))(x)\n",
        "    x = Conv2D(kernel_size=(1,1),filters=out_channels,use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    res.append(x)\n",
        "    x = Add()(res)\n",
        "    x = Conv2D(kernel_size=(1,1),filters=out_channels,use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-uDG4ByNYTi"
      },
      "outputs": [],
      "source": [
        "def deeplab_head(input,num_classes):\n",
        "    x = ASPP(input,[12,24,36],256)\n",
        "    x = Conv2D(256,(3,3),padding=\"same\",use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = UpSampling2D((224//x.shape[1],224//x.shape[2]),interpolation=\"bilinear\")(x)\n",
        "    x = Conv2D(num_classes,(1,1),padding=\"same\")(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRITGFgeOUIh"
      },
      "outputs": [],
      "source": [
        "def FCN_head(input,in_channels,out_channels):\n",
        "    inter_channels = in_channels//4\n",
        "    x = Conv2D(inter_channels,(3,3),padding=\"same\",use_bias=False)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(rate=0.1)(x)\n",
        "    x = Conv2D(out_channels,(1,1))(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXgrttgyPkPe"
      },
      "outputs": [],
      "source": [
        "def build_deeplab_decoder(input):\n",
        "    #x = FCN_head(input,1024,num_classes)\n",
        "    x = deeplab_head(input,num_classes)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyL5f-_UZzie"
      },
      "outputs": [],
      "source": [
        "def build_encoder(input_layer,num):\n",
        "    input_shape = (224,224)\n",
        "    layers = []\n",
        "    input_layer = input_layer\n",
        "    bn_axis = 3\n",
        "    x = ZeroPadding2D((3,3),data_format=\"channels_last\")(input_layer)\n",
        "    x = Conv2D(64,(7,7),(2,2),data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    layers.append(x)\n",
        "    x = MaxPool2D((3,3),(2,2),data_format=\"channels_last\")(x)\n",
        "    x = conv_block(x,3,(64,64,256),(1,1))\n",
        "    x = identity_block(x,3,(64,64,256))\n",
        "    x = identity_block(x,3,(64,64,256))\n",
        "    layers.append(x)\n",
        "\n",
        "    x = conv_block(x,3,(128,128,512))\n",
        "    x = identity_block(x,3,(128,128,512))\n",
        "    x = identity_block(x,3,(128,128,512))\n",
        "    x = identity_block(x,3,(128,128,512))\n",
        "    layers.append(x)\n",
        "\n",
        "    x = conv_block(x,3,(256,256,1024))\n",
        "    x = identity_block(x,3,(256,256,1024))\n",
        "    x = identity_block(x,3,(256,256,1024))\n",
        "    x = identity_block(x,3,(256,256,1024))\n",
        "    x = identity_block(x,3,(256,256,1024))\n",
        "    x = identity_block(x,3,(256,256,1024))\n",
        "    layers.append(x)\n",
        "\n",
        "    x = conv_block(x,3,(512,512,2048))\n",
        "    x = identity_block(x,3,(512,512,2048))\n",
        "    x = identity_block(x,3,(512,512,2048))\n",
        "    layers.append(x)\n",
        "\n",
        "    #x = AveragePooling2D((7,7),data_format=\"channels_last\")(x)\n",
        "    #model = Model(inputs=input_layer,outputs=x)\n",
        "    return layers[num]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im9nhvyUfiNQ"
      },
      "outputs": [],
      "source": [
        "def build_decoder(input):\n",
        "    n_classes = 2\n",
        "    bn_axis = 3\n",
        "    x = ZeroPadding2D((1,1),data_format=\"channels_last\")(input)\n",
        "    x = Conv2D(512,(3,3),padding=\"valid\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = UpSampling2D((2,2),data_format=\"channels_last\")(x)\n",
        "    x = ZeroPadding2D((1,1),data_format=\"channels_last\")(x)\n",
        "    x = Conv2D(256,(3,3),padding=\"valid\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = UpSampling2D((2,2),data_format=\"channels_last\")(x)\n",
        "    x = ZeroPadding2D((1,1),data_format=\"channels_last\")(x)\n",
        "    x = Conv2D(128,(3,3),padding=\"valid\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = UpSampling2D((4,4),data_format=\"channels_last\")(x)\n",
        "    x = ZeroPadding2D((1,1),data_format=\"channels_last\")(x)\n",
        "    x = Conv2D(64,(3,3),padding=\"valid\",data_format=\"channels_last\")(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Conv2D(n_classes,(3,3),padding=\"same\",data_format=\"channels_last\")(x)\n",
        "    #model = Model(inputs=input,outputs=x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR5Tdp8zPVM3"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def get_intersection_and_sums(prediction,target):\n",
        "    p = tf.cast(K.batch_flatten(prediction),tf.float32)\n",
        "    t = tf.cast(K.batch_flatten(K.argmax(target,axis=-1)),tf.float32)\n",
        "    print(p.shape,t.shape,sep=\" \")\n",
        "    intersection = K.sum(p * t,axis=-1)\n",
        "    output_sum = K.sum(p,axis=-1)\n",
        "    target_sum = K.sum(t,axis=-1)\n",
        "    return intersection, output_sum, target_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdnK4DUTPVM3"
      },
      "outputs": [],
      "source": [
        "def dice_coef(prediction, target):\n",
        "    intersection, output_sum, target_sum = get_intersection_and_sums(prediction, target)\n",
        "    dice = 2 * intersection / (output_sum + target_sum)\n",
        "    return dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50JFYBu-PVM3"
      },
      "outputs": [],
      "source": [
        "def dice_coef1(y_true, y_pred, smooth=1):\n",
        "    y_true_f = K.batch_flatten(y_true)\n",
        "    y_pred_f = K.batch_flatten(y_pred)\n",
        "    y_pred_f = K.argmax(y_pred_f,axis=-1)\n",
        "    y_pred_f = tf.cast(y_pred_f,tf.float32)\n",
        "    print(y_true_f.shape,y_pred_f.shape,sep=\" \")\n",
        "    intersection = K.sum(y_true_f * y_pred_f, axis=-1)\n",
        "    sums = K.sum(y_true_f, axis=-1) + K.sum(y_pred_f, axis=-1)\n",
        "    return (2. * intersection + smooth) / (sums + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYtCUJGZi3aE"
      },
      "outputs": [],
      "source": [
        "def build_network():\n",
        "    input_layer = Input(shape=(224,224,3))\n",
        "    output = build_encoder(input_layer,1)\n",
        "    #output = build_decoder(output)\n",
        "    output = build_deeplab_decoder(output)\n",
        "    model = Model(inputs=input_layer,outputs=output)\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3,epsilon=1e-8),metrics=[\"accuracy\",dice_coef])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAhss4xWdLLw"
      },
      "outputs": [],
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input,hp):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    dilation_rate = hp.Int(\"dilation_rate\",min_value=2,max_value=12)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate*2)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate*3)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G8c8eGGdQxK"
      },
      "outputs": [],
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    #trainable_layers = hp.Int(\"trainable_layers\"\n",
        "    for layer in resnet50.layers:\n",
        "        layer.trainable = True\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPCzQauKOjO7"
      },
      "outputs": [],
      "source": [
        "def DeeplabV3Plus1(hp):\n",
        "    image_size = 224\n",
        "    num_classes = 2\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    trainable_layers = hp.Int(\"trainable_layers\",min_value=0,max_value=len(resnet50.layers))\n",
        "    for idx,layer in enumerate(reversed(resnet50.layers)):\n",
        "        if idx == trainable_layers:\n",
        "            break\n",
        "        layer.trainable = True\n",
        "    x = resnet50.get_layer(\"conv3_block3_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x,hp)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    model = keras.Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-8),metrics=[\"accuracy\",dice_coef])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW43siYToK2E"
      },
      "outputs": [],
      "source": [
        "def load_avi(path, max_frames=0):\n",
        "    capture = cv2.VideoCapture(\"Videos/\" + path + \".avi\")\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = capture.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = frame[:,:,[2,1,0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        capture.release()\n",
        "    for i in range(len(frames)):\n",
        "        frame = frames[i]\n",
        "        frames[i] = cv2.resize(frame,dsize=image_size[:2],interpolation=cv2.INTER_CUBIC)\n",
        "    return np.array(frames) / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3D5-QbYoMVQ"
      },
      "outputs": [],
      "source": [
        "def sequentialize(frame):\n",
        "    coords1 = frame[[\"X1\",\"Y1\"]].values\n",
        "    coords2 = np.flip(frame[[\"X2\",\"Y2\"]].iloc[1:].values,axis=0)\n",
        "    bottom_coord = frame[[\"X2\",\"Y2\"]].iloc[0].values\n",
        "    coords = np.vstack([coords1,bottom_coord,coords2])\n",
        "    return coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyaBaCpWoPwk"
      },
      "outputs": [],
      "source": [
        "def yield_segmentation(filename):\n",
        "    frame = vols.loc[(vols[\"FileName\"] == filename + \".avi\"),:]\n",
        "    try:\n",
        "        mask = frame[\"Frame\"] == frame[\"Frame\"].iloc[0]\n",
        "    except:\n",
        "        print(frame,filename)\n",
        "    first = frame[mask]\n",
        "    num1 = first[\"Frame\"].iloc[0]\n",
        "    second = frame[~mask]\n",
        "    num2 = second[\"Frame\"].iloc[0]\n",
        "    coords_first = sequentialize(first)\n",
        "    coords_second = sequentialize(second)\n",
        "    r1,c1 = polygon(coords_first[:,1],coords_first[:,0])\n",
        "    r1,c1 = np.clip(r1,0,111),np.clip(c1,0,111)\n",
        "    r2,c2 = polygon(coords_second[:,1],coords_second[:,0])\n",
        "    r2,c2 = np.clip(r2,0,111),np.clip(c2,0,111)\n",
        "    seg1 = np.zeros((112,112),dtype=np.uint8)\n",
        "    seg1[r1,c1] = 1\n",
        "    seg1 = cv2.resize(seg1,dsize=image_size[:2],interpolation=cv2.INTER_CUBIC)\n",
        "    seg2 = np.zeros((112,112),dtype=np.uint8)\n",
        "    seg2[r2,c2] = 1\n",
        "    seg2 = cv2.resize(seg2,dsize=image_size[:2],interpolation=cv2.INTER_CUBIC)\n",
        "    video = load_avi(filename)\n",
        "    return (video[num1],seg1),(video[num2],seg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4k2eEghqU8d"
      },
      "outputs": [],
      "source": [
        "def prepare_data(df,root_dir):\n",
        "    video_paths = df[\"FileName\"].values.tolist()\n",
        "    num_samples = len(df)*2\n",
        "    image = np.zeros(shape=(num_samples,224,224,3),dtype=\"float32\")\n",
        "    mask = np.zeros(shape=(num_samples,224,224,1),dtype=\"float32\")\n",
        "    for idx,video_path in enumerate(video_paths):\n",
        "        temp_image = np.zeros(shape=(2,224,224,3),dtype=\"float32\")\n",
        "        temp_mask = np.zeros(shape=(2,224,224,1),dtype=\"float32\")\n",
        "        x1,x2 = yield_segmentation(video_path)\n",
        "        temp_image[0,:,:,:] = x1[0]\n",
        "        temp_mask[0,:,:] = np.expand_dims(x1[1],axis=-1)\n",
        "        temp_image[1,:,:,:] = x2[0]\n",
        "        temp_mask[1,:,:] = np.expand_dims(x2[1],axis=-1)\n",
        "        image[2*idx:(2*idx)+2,] = temp_image\n",
        "        mask[2*idx:(2*idx)+2,] = temp_mask\n",
        "    return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QHLY2hznG9-"
      },
      "outputs": [],
      "source": [
        "class PrepareImagesGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self,df,batch_size):\n",
        "        self.df = df.copy()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = image_size\n",
        "        self.n = len(self.df)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        batches = self.df[(index*self.batch_size):((index+1)*self.batch_size)]\n",
        "        X,y = prepare_data(batches,path)\n",
        "        #print(y.shape)\n",
        "        return X,y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n//self.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfE1Ptu1PVM6",
        "outputId": "e276c04c-3980-4321-cb6c-8d522b1555f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [10h 02m 27s]\n",
            "val_dice_coef: 0.8940598368644714\n",
            "\n",
            "Best val_dice_coef So Far: 0.8940598368644714\n",
            "Total elapsed time: 1d 06h 06m 40s\n",
            "\n",
            "Search: Running Trial #4\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "96                |156               |trainable_layers\n",
            "2                 |5                 |dilation_rate\n",
            "\n",
            "Epoch 1/10\n",
            "(None, None) (None, None)\n",
            "(None, None) (None, None)\n",
            "626/932 [===================>..........] - ETA: 17:46 - loss: 0.0738 - accuracy: 0.9059 - dice_coef: 0.8460"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "932/932 [==============================] - 3591s 4s/step - loss: 0.0317 - accuracy: 0.9111 - dice_coef: 0.9028 - val_loss: 0.0458 - val_accuracy: 0.9129 - val_dice_coef: 0.8878 - lr: 9.7553e-05\n",
            "Epoch 7/10\n",
            "932/932 [==============================] - 3589s 4s/step - loss: 0.0298 - accuracy: 0.9112 - dice_coef: 0.9068 - val_loss: 0.0412 - val_accuracy: 0.9098 - val_dice_coef: 0.8804 - lr: 9.6489e-05\n",
            "Epoch 8/10\n",
            "932/932 [==============================] - 3590s 4s/step - loss: 0.0279 - accuracy: 0.9113 - dice_coef: 0.9116 - val_loss: 0.0431 - val_accuracy: 0.9049 - val_dice_coef: 0.8884 - lr: 9.5241e-05\n",
            "Epoch 9/10\n",
            "266/932 [=======>......................] - ETA: 39:43 - loss: 0.0258 - accuracy: 0.9107 - dice_coef: 0.9183"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "932/932 [==============================] - 4041s 4s/step - loss: 0.0397 - accuracy: 0.9102 - dice_coef: 0.8885 - val_loss: 0.0442 - val_accuracy: 0.9173 - val_dice_coef: 0.8715 - lr: 9.9606e-05\n",
            "Epoch 4/10\n",
            "932/932 [==============================] - 4046s 4s/step - loss: 0.0374 - accuracy: 0.9104 - dice_coef: 0.8927 - val_loss: 0.0444 - val_accuracy: 0.9043 - val_dice_coef: 0.8884 - lr: 9.9114e-05\n",
            "Epoch 5/10\n",
            "932/932 [==============================] - 4067s 4s/step - loss: 0.0356 - accuracy: 0.9106 - dice_coef: 0.8956 - val_loss: 0.0405 - val_accuracy: 0.9084 - val_dice_coef: 0.8918 - lr: 9.8429e-05\n",
            "Epoch 6/10\n",
            "932/932 [==============================] - 4053s 4s/step - loss: 0.0337 - accuracy: 0.9109 - dice_coef: 0.8988 - val_loss: 0.0421 - val_accuracy: 0.9101 - val_dice_coef: 0.8808 - lr: 9.7553e-05\n",
            "Epoch 7/10\n",
            "106/932 [==>...........................] - ETA: 55:00 - loss: 0.0317 - accuracy: 0.9131 - dice_coef: 0.8981"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "932/932 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9082 - dice_coef: 0.8513(None, None) (None, None)\n",
            "932/932 [==============================] - 3959s 4s/step - loss: 0.0666 - accuracy: 0.9082 - dice_coef: 0.8513 - val_loss: 0.0956 - val_accuracy: 0.9306 - val_dice_coef: 0.7467 - lr: 1.0000e-04\n",
            "Epoch 2/10\n",
            "932/932 [==============================] - 3965s 4s/step - loss: 0.0425 - accuracy: 0.9102 - dice_coef: 0.8839 - val_loss: 0.0433 - val_accuracy: 0.9087 - val_dice_coef: 0.8885 - lr: 9.9901e-05\n",
            "Epoch 3/10\n",
            "932/932 [==============================] - 3955s 4s/step - loss: 0.0392 - accuracy: 0.9103 - dice_coef: 0.8895 - val_loss: 0.0439 - val_accuracy: 0.9073 - val_dice_coef: 0.8834 - lr: 9.9606e-05\n",
            "Epoch 4/10\n",
            "932/932 [==============================] - 3946s 4s/step - loss: 0.0369 - accuracy: 0.9105 - dice_coef: 0.8930 - val_loss: 0.0417 - val_accuracy: 0.9042 - val_dice_coef: 0.8917 - lr: 9.9114e-05\n",
            "Epoch 5/10\n",
            "932/932 [==============================] - 3945s 4s/step - loss: 0.0350 - accuracy: 0.9107 - dice_coef: 0.8959 - val_loss: 0.0441 - val_accuracy: 0.9125 - val_dice_coef: 0.8718 - lr: 9.8429e-05\n",
            "Epoch 6/10\n",
            "932/932 [==============================] - 3948s 4s/step - loss: 0.0335 - accuracy: 0.9109 - dice_coef: 0.8987 - val_loss: 0.0409 - val_accuracy: 0.9131 - val_dice_coef: 0.8855 - lr: 9.7553e-05\n",
            "Epoch 7/10\n",
            "932/932 [==============================] - 3931s 4s/step - loss: 0.0316 - accuracy: 0.9112 - dice_coef: 0.9024 - val_loss: 0.0433 - val_accuracy: 0.9154 - val_dice_coef: 0.8739 - lr: 9.6489e-05\n",
            "Epoch 8/10\n",
            "932/932 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9067 - dice_coef: 0.8496(None, None) (None, None)\n",
            "932/932 [==============================] - 4041s 4s/step - loss: 0.0704 - accuracy: 0.9067 - dice_coef: 0.8496 - val_loss: 0.1461 - val_accuracy: 0.8943 - val_dice_coef: 0.8002 - lr: 1.0000e-04\n",
            "Epoch 2/10\n",
            "932/932 [==============================] - 3962s 4s/step - loss: 0.0394 - accuracy: 0.9103 - dice_coef: 0.8894 - val_loss: 0.0419 - val_accuracy: 0.9123 - val_dice_coef: 0.8849 - lr: 9.9606e-05\n",
            "Epoch 4/10\n",
            "932/932 [==============================] - 3960s 4s/step - loss: 0.0373 - accuracy: 0.9104 - dice_coef: 0.8928 - val_loss: 0.0400 - val_accuracy: 0.9110 - val_dice_coef: 0.8904 - lr: 9.9114e-05\n",
            "Epoch 5/10\n",
            "932/932 [==============================] - 3948s 4s/step - loss: 0.0355 - accuracy: 0.9107 - dice_coef: 0.8953 - val_loss: 0.0404 - val_accuracy: 0.9062 - val_dice_coef: 0.8912 - lr: 9.8429e-05\n",
            "Epoch 6/10\n",
            "932/932 [==============================] - 3963s 4s/step - loss: 0.0338 - accuracy: 0.9109 - dice_coef: 0.8986 - val_loss: 0.0432 - val_accuracy: 0.9049 - val_dice_coef: 0.8852 - lr: 9.7553e-05\n",
            "Epoch 7/10\n",
            "932/932 [==============================] - 4077s 4s/step - loss: 0.0390 - accuracy: 0.9103 - dice_coef: 0.8900 - val_loss: 0.0441 - val_accuracy: 0.9099 - val_dice_coef: 0.8854 - lr: 9.9606e-05\n",
            "Epoch 4/10\n",
            "932/932 [==============================] - 3907s 4s/step - loss: 0.0368 - accuracy: 0.9105 - dice_coef: 0.8935 - val_loss: 0.0419 - val_accuracy: 0.9135 - val_dice_coef: 0.8842 - lr: 9.9114e-05\n",
            "Epoch 5/10\n",
            "249/932 [=======>......................] - ETA: 44:12 - loss: 0.0340 - accuracy: 0.9109 - dice_coef: 0.8983"
          ]
        }
      ],
      "source": [
        "train_gen = PrepareImagesGen(metadata.loc[metadata[\"Split\"] == \"TRAIN\",:],batch_size=batch_size)\n",
        "val_gen = PrepareImagesGen(metadata.loc[metadata[\"Split\"] == \"VAL\",:],batch_size=batch_size)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"models/segmenter.{epoch:02d}-{val_loss:.2f}.hdf5\",save_weights_only=True,save_best_only=False)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(tf.keras.optimizers.schedules.CosineDecay(learning_rate,50),verbose=0)\n",
        "model = kt.BayesianOptimization(DeeplabV3Plus1,objective=kt.Objective(\"val_dice_coef\", direction=\"max\"),max_trials=21,seed=42,directory=\"models/\")\n",
        "history = model.search(train_gen,epochs=epochs,callbacks=[checkpoint,scheduler],validation_data=val_gen)\n",
        "besthp = model.get_best_hyperparameters()[0]\n",
        "print(besthp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def inspect_hdf5(file_path):\n",
        "    with h5py.File(file_path, 'r') as file:\n",
        "        # Print the keys at the root level\n",
        "        print(\"Keys at the root level:\", list(file.keys()))\n",
        "\n",
        "        # Print the attributes at the root level\n",
        "        print(\"Attributes at the root level:\", dict(file.attrs))\n",
        "\n",
        "        # Recursively print the keys and attributes in the file\n",
        "        def print_recursive(name, obj):\n",
        "            print(name)\n",
        "            for key, val in obj.attrs.items():\n",
        "                print(\"    {}: {}\".format(key, val))\n",
        "\n",
        "        file.visititems(print_recursive)\n",
        "\n",
        "file_path = 'SegModels/segmenter.06-0.04.hdf5'\n",
        "inspect_hdf5(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaUiafveUlkt",
        "outputId": "50614e81-249f-4238-fe8f-9f06913cc232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys at the root level: ['average_pooling2d', 'batch_normalization', 'batch_normalization_1', 'batch_normalization_2', 'batch_normalization_3', 'batch_normalization_4', 'batch_normalization_5', 'batch_normalization_6', 'batch_normalization_7', 'batch_normalization_8', 'concatenate', 'concatenate_1', 'conv1_bn', 'conv1_conv', 'conv1_pad', 'conv1_relu', 'conv2_block1_0_bn', 'conv2_block1_0_conv', 'conv2_block1_1_bn', 'conv2_block1_1_conv', 'conv2_block1_1_relu', 'conv2_block1_2_bn', 'conv2_block1_2_conv', 'conv2_block1_2_relu', 'conv2_block1_3_bn', 'conv2_block1_3_conv', 'conv2_block1_add', 'conv2_block1_out', 'conv2_block2_1_bn', 'conv2_block2_1_conv', 'conv2_block2_1_relu', 'conv2_block2_2_bn', 'conv2_block2_2_conv', 'conv2_block2_2_relu', 'conv2_block2_3_bn', 'conv2_block2_3_conv', 'conv2_block2_add', 'conv2_block2_out', 'conv2_block3_1_bn', 'conv2_block3_1_conv', 'conv2_block3_1_relu', 'conv2_block3_2_bn', 'conv2_block3_2_conv', 'conv2_block3_2_relu', 'conv2_block3_3_bn', 'conv2_block3_3_conv', 'conv2_block3_add', 'conv2_block3_out', 'conv2d', 'conv2d_1', 'conv2d_2', 'conv2d_3', 'conv2d_4', 'conv2d_5', 'conv2d_6', 'conv2d_7', 'conv2d_8', 'conv2d_9', 'conv3_block1_0_bn', 'conv3_block1_0_conv', 'conv3_block1_1_bn', 'conv3_block1_1_conv', 'conv3_block1_1_relu', 'conv3_block1_2_bn', 'conv3_block1_2_conv', 'conv3_block1_2_relu', 'conv3_block1_3_bn', 'conv3_block1_3_conv', 'conv3_block1_add', 'conv3_block1_out', 'conv3_block2_1_bn', 'conv3_block2_1_conv', 'conv3_block2_1_relu', 'conv3_block2_2_bn', 'conv3_block2_2_conv', 'conv3_block2_2_relu', 'conv3_block2_3_bn', 'conv3_block2_3_conv', 'conv3_block2_add', 'conv3_block2_out', 'conv3_block3_1_bn', 'conv3_block3_1_conv', 'conv3_block3_1_relu', 'conv3_block3_2_bn', 'conv3_block3_2_conv', 'conv3_block3_2_relu', 'input_1', 'pool1_pad', 'pool1_pool', 'tf.nn.relu', 'tf.nn.relu_1', 'tf.nn.relu_2', 'tf.nn.relu_3', 'tf.nn.relu_4', 'tf.nn.relu_5', 'tf.nn.relu_6', 'tf.nn.relu_7', 'tf.nn.relu_8', 'top_level_model_weights', 'up_sampling2d', 'up_sampling2d_1', 'up_sampling2d_2']\n",
            "Attributes at the root level: {'backend': 'tensorflow', 'keras_version': '2.11.0', 'layer_names': array([b'input_1', b'conv1_pad', b'conv1_conv', b'conv1_bn',\n",
            "       b'conv1_relu', b'pool1_pad', b'pool1_pool', b'conv2_block1_1_conv',\n",
            "       b'conv2_block1_1_bn', b'conv2_block1_1_relu',\n",
            "       b'conv2_block1_2_conv', b'conv2_block1_2_bn',\n",
            "       b'conv2_block1_2_relu', b'conv2_block1_0_conv',\n",
            "       b'conv2_block1_3_conv', b'conv2_block1_0_bn', b'conv2_block1_3_bn',\n",
            "       b'conv2_block1_add', b'conv2_block1_out', b'conv2_block2_1_conv',\n",
            "       b'conv2_block2_1_bn', b'conv2_block2_1_relu',\n",
            "       b'conv2_block2_2_conv', b'conv2_block2_2_bn',\n",
            "       b'conv2_block2_2_relu', b'conv2_block2_3_conv',\n",
            "       b'conv2_block2_3_bn', b'conv2_block2_add', b'conv2_block2_out',\n",
            "       b'conv2_block3_1_conv', b'conv2_block3_1_bn',\n",
            "       b'conv2_block3_1_relu', b'conv2_block3_2_conv',\n",
            "       b'conv2_block3_2_bn', b'conv2_block3_2_relu',\n",
            "       b'conv2_block3_3_conv', b'conv2_block3_3_bn', b'conv2_block3_add',\n",
            "       b'conv2_block3_out', b'conv3_block1_1_conv', b'conv3_block1_1_bn',\n",
            "       b'conv3_block1_1_relu', b'conv3_block1_2_conv',\n",
            "       b'conv3_block1_2_bn', b'conv3_block1_2_relu',\n",
            "       b'conv3_block1_0_conv', b'conv3_block1_3_conv',\n",
            "       b'conv3_block1_0_bn', b'conv3_block1_3_bn', b'conv3_block1_add',\n",
            "       b'conv3_block1_out', b'conv3_block2_1_conv', b'conv3_block2_1_bn',\n",
            "       b'conv3_block2_1_relu', b'conv3_block2_2_conv',\n",
            "       b'conv3_block2_2_bn', b'conv3_block2_2_relu',\n",
            "       b'conv3_block2_3_conv', b'conv3_block2_3_bn', b'conv3_block2_add',\n",
            "       b'conv3_block2_out', b'conv3_block3_1_conv', b'conv3_block3_1_bn',\n",
            "       b'conv3_block3_1_relu', b'conv3_block3_2_conv',\n",
            "       b'conv3_block3_2_bn', b'conv3_block3_2_relu', b'average_pooling2d',\n",
            "       b'conv2d', b'batch_normalization', b'conv2d_1', b'conv2d_2',\n",
            "       b'conv2d_3', b'conv2d_4', b'tf.nn.relu', b'batch_normalization_1',\n",
            "       b'batch_normalization_2', b'batch_normalization_3',\n",
            "       b'batch_normalization_4', b'up_sampling2d', b'tf.nn.relu_1',\n",
            "       b'tf.nn.relu_2', b'tf.nn.relu_3', b'tf.nn.relu_4', b'concatenate',\n",
            "       b'conv2d_5', b'batch_normalization_5', b'conv2d_6',\n",
            "       b'tf.nn.relu_5', b'batch_normalization_6', b'up_sampling2d_1',\n",
            "       b'tf.nn.relu_6', b'concatenate_1', b'conv2d_7',\n",
            "       b'batch_normalization_7', b'tf.nn.relu_7', b'conv2d_8',\n",
            "       b'batch_normalization_8', b'tf.nn.relu_8', b'up_sampling2d_2',\n",
            "       b'conv2d_9'], dtype='|S21')}\n",
            "average_pooling2d\n",
            "    weight_names: []\n",
            "batch_normalization\n",
            "    weight_names: [b'batch_normalization/gamma:0' b'batch_normalization/beta:0'\n",
            " b'batch_normalization/moving_mean:0'\n",
            " b'batch_normalization/moving_variance:0']\n",
            "batch_normalization/batch_normalization\n",
            "batch_normalization/batch_normalization/beta:0\n",
            "batch_normalization/batch_normalization/gamma:0\n",
            "batch_normalization/batch_normalization/moving_mean:0\n",
            "batch_normalization/batch_normalization/moving_variance:0\n",
            "batch_normalization_1\n",
            "    weight_names: [b'batch_normalization_1/gamma:0' b'batch_normalization_1/beta:0'\n",
            " b'batch_normalization_1/moving_mean:0'\n",
            " b'batch_normalization_1/moving_variance:0']\n",
            "batch_normalization_1/batch_normalization_1\n",
            "batch_normalization_1/batch_normalization_1/beta:0\n",
            "batch_normalization_1/batch_normalization_1/gamma:0\n",
            "batch_normalization_1/batch_normalization_1/moving_mean:0\n",
            "batch_normalization_1/batch_normalization_1/moving_variance:0\n",
            "batch_normalization_2\n",
            "    weight_names: [b'batch_normalization_2/gamma:0' b'batch_normalization_2/beta:0'\n",
            " b'batch_normalization_2/moving_mean:0'\n",
            " b'batch_normalization_2/moving_variance:0']\n",
            "batch_normalization_2/batch_normalization_2\n",
            "batch_normalization_2/batch_normalization_2/beta:0\n",
            "batch_normalization_2/batch_normalization_2/gamma:0\n",
            "batch_normalization_2/batch_normalization_2/moving_mean:0\n",
            "batch_normalization_2/batch_normalization_2/moving_variance:0\n",
            "batch_normalization_3\n",
            "    weight_names: [b'batch_normalization_3/gamma:0' b'batch_normalization_3/beta:0'\n",
            " b'batch_normalization_3/moving_mean:0'\n",
            " b'batch_normalization_3/moving_variance:0']\n",
            "batch_normalization_3/batch_normalization_3\n",
            "batch_normalization_3/batch_normalization_3/beta:0\n",
            "batch_normalization_3/batch_normalization_3/gamma:0\n",
            "batch_normalization_3/batch_normalization_3/moving_mean:0\n",
            "batch_normalization_3/batch_normalization_3/moving_variance:0\n",
            "batch_normalization_4\n",
            "    weight_names: [b'batch_normalization_4/gamma:0' b'batch_normalization_4/beta:0'\n",
            " b'batch_normalization_4/moving_mean:0'\n",
            " b'batch_normalization_4/moving_variance:0']\n",
            "batch_normalization_4/batch_normalization_4\n",
            "batch_normalization_4/batch_normalization_4/beta:0\n",
            "batch_normalization_4/batch_normalization_4/gamma:0\n",
            "batch_normalization_4/batch_normalization_4/moving_mean:0\n",
            "batch_normalization_4/batch_normalization_4/moving_variance:0\n",
            "batch_normalization_5\n",
            "    weight_names: [b'batch_normalization_5/gamma:0' b'batch_normalization_5/beta:0'\n",
            " b'batch_normalization_5/moving_mean:0'\n",
            " b'batch_normalization_5/moving_variance:0']\n",
            "batch_normalization_5/batch_normalization_5\n",
            "batch_normalization_5/batch_normalization_5/beta:0\n",
            "batch_normalization_5/batch_normalization_5/gamma:0\n",
            "batch_normalization_5/batch_normalization_5/moving_mean:0\n",
            "batch_normalization_5/batch_normalization_5/moving_variance:0\n",
            "batch_normalization_6\n",
            "    weight_names: [b'batch_normalization_6/gamma:0' b'batch_normalization_6/beta:0'\n",
            " b'batch_normalization_6/moving_mean:0'\n",
            " b'batch_normalization_6/moving_variance:0']\n",
            "batch_normalization_6/batch_normalization_6\n",
            "batch_normalization_6/batch_normalization_6/beta:0\n",
            "batch_normalization_6/batch_normalization_6/gamma:0\n",
            "batch_normalization_6/batch_normalization_6/moving_mean:0\n",
            "batch_normalization_6/batch_normalization_6/moving_variance:0\n",
            "batch_normalization_7\n",
            "    weight_names: [b'batch_normalization_7/gamma:0' b'batch_normalization_7/beta:0'\n",
            " b'batch_normalization_7/moving_mean:0'\n",
            " b'batch_normalization_7/moving_variance:0']\n",
            "batch_normalization_7/batch_normalization_7\n",
            "batch_normalization_7/batch_normalization_7/beta:0\n",
            "batch_normalization_7/batch_normalization_7/gamma:0\n",
            "batch_normalization_7/batch_normalization_7/moving_mean:0\n",
            "batch_normalization_7/batch_normalization_7/moving_variance:0\n",
            "batch_normalization_8\n",
            "    weight_names: [b'batch_normalization_8/gamma:0' b'batch_normalization_8/beta:0'\n",
            " b'batch_normalization_8/moving_mean:0'\n",
            " b'batch_normalization_8/moving_variance:0']\n",
            "batch_normalization_8/batch_normalization_8\n",
            "batch_normalization_8/batch_normalization_8/beta:0\n",
            "batch_normalization_8/batch_normalization_8/gamma:0\n",
            "batch_normalization_8/batch_normalization_8/moving_mean:0\n",
            "batch_normalization_8/batch_normalization_8/moving_variance:0\n",
            "concatenate\n",
            "    weight_names: []\n",
            "concatenate_1\n",
            "    weight_names: []\n",
            "conv1_bn\n",
            "    weight_names: [b'conv1_bn/gamma:0' b'conv1_bn/beta:0' b'conv1_bn/moving_mean:0'\n",
            " b'conv1_bn/moving_variance:0']\n",
            "conv1_bn/conv1_bn\n",
            "conv1_bn/conv1_bn/beta:0\n",
            "conv1_bn/conv1_bn/gamma:0\n",
            "conv1_bn/conv1_bn/moving_mean:0\n",
            "conv1_bn/conv1_bn/moving_variance:0\n",
            "conv1_conv\n",
            "    weight_names: [b'conv1_conv/kernel:0' b'conv1_conv/bias:0']\n",
            "conv1_conv/conv1_conv\n",
            "conv1_conv/conv1_conv/bias:0\n",
            "conv1_conv/conv1_conv/kernel:0\n",
            "conv1_pad\n",
            "    weight_names: []\n",
            "conv1_relu\n",
            "    weight_names: []\n",
            "conv2_block1_0_bn\n",
            "    weight_names: [b'conv2_block1_0_bn/gamma:0' b'conv2_block1_0_bn/beta:0'\n",
            " b'conv2_block1_0_bn/moving_mean:0' b'conv2_block1_0_bn/moving_variance:0']\n",
            "conv2_block1_0_bn/conv2_block1_0_bn\n",
            "conv2_block1_0_bn/conv2_block1_0_bn/beta:0\n",
            "conv2_block1_0_bn/conv2_block1_0_bn/gamma:0\n",
            "conv2_block1_0_bn/conv2_block1_0_bn/moving_mean:0\n",
            "conv2_block1_0_bn/conv2_block1_0_bn/moving_variance:0\n",
            "conv2_block1_0_conv\n",
            "    weight_names: [b'conv2_block1_0_conv/kernel:0' b'conv2_block1_0_conv/bias:0']\n",
            "conv2_block1_0_conv/conv2_block1_0_conv\n",
            "conv2_block1_0_conv/conv2_block1_0_conv/bias:0\n",
            "conv2_block1_0_conv/conv2_block1_0_conv/kernel:0\n",
            "conv2_block1_1_bn\n",
            "    weight_names: [b'conv2_block1_1_bn/gamma:0' b'conv2_block1_1_bn/beta:0'\n",
            " b'conv2_block1_1_bn/moving_mean:0' b'conv2_block1_1_bn/moving_variance:0']\n",
            "conv2_block1_1_bn/conv2_block1_1_bn\n",
            "conv2_block1_1_bn/conv2_block1_1_bn/beta:0\n",
            "conv2_block1_1_bn/conv2_block1_1_bn/gamma:0\n",
            "conv2_block1_1_bn/conv2_block1_1_bn/moving_mean:0\n",
            "conv2_block1_1_bn/conv2_block1_1_bn/moving_variance:0\n",
            "conv2_block1_1_conv\n",
            "    weight_names: [b'conv2_block1_1_conv/kernel:0' b'conv2_block1_1_conv/bias:0']\n",
            "conv2_block1_1_conv/conv2_block1_1_conv\n",
            "conv2_block1_1_conv/conv2_block1_1_conv/bias:0\n",
            "conv2_block1_1_conv/conv2_block1_1_conv/kernel:0\n",
            "conv2_block1_1_relu\n",
            "    weight_names: []\n",
            "conv2_block1_2_bn\n",
            "    weight_names: [b'conv2_block1_2_bn/gamma:0' b'conv2_block1_2_bn/beta:0'\n",
            " b'conv2_block1_2_bn/moving_mean:0' b'conv2_block1_2_bn/moving_variance:0']\n",
            "conv2_block1_2_bn/conv2_block1_2_bn\n",
            "conv2_block1_2_bn/conv2_block1_2_bn/beta:0\n",
            "conv2_block1_2_bn/conv2_block1_2_bn/gamma:0\n",
            "conv2_block1_2_bn/conv2_block1_2_bn/moving_mean:0\n",
            "conv2_block1_2_bn/conv2_block1_2_bn/moving_variance:0\n",
            "conv2_block1_2_conv\n",
            "    weight_names: [b'conv2_block1_2_conv/kernel:0' b'conv2_block1_2_conv/bias:0']\n",
            "conv2_block1_2_conv/conv2_block1_2_conv\n",
            "conv2_block1_2_conv/conv2_block1_2_conv/bias:0\n",
            "conv2_block1_2_conv/conv2_block1_2_conv/kernel:0\n",
            "conv2_block1_2_relu\n",
            "    weight_names: []\n",
            "conv2_block1_3_bn\n",
            "    weight_names: [b'conv2_block1_3_bn/gamma:0' b'conv2_block1_3_bn/beta:0'\n",
            " b'conv2_block1_3_bn/moving_mean:0' b'conv2_block1_3_bn/moving_variance:0']\n",
            "conv2_block1_3_bn/conv2_block1_3_bn\n",
            "conv2_block1_3_bn/conv2_block1_3_bn/beta:0\n",
            "conv2_block1_3_bn/conv2_block1_3_bn/gamma:0\n",
            "conv2_block1_3_bn/conv2_block1_3_bn/moving_mean:0\n",
            "conv2_block1_3_bn/conv2_block1_3_bn/moving_variance:0\n",
            "conv2_block1_3_conv\n",
            "    weight_names: [b'conv2_block1_3_conv/kernel:0' b'conv2_block1_3_conv/bias:0']\n",
            "conv2_block1_3_conv/conv2_block1_3_conv\n",
            "conv2_block1_3_conv/conv2_block1_3_conv/bias:0\n",
            "conv2_block1_3_conv/conv2_block1_3_conv/kernel:0\n",
            "conv2_block1_add\n",
            "    weight_names: []\n",
            "conv2_block1_out\n",
            "    weight_names: []\n",
            "conv2_block2_1_bn\n",
            "    weight_names: [b'conv2_block2_1_bn/gamma:0' b'conv2_block2_1_bn/beta:0'\n",
            " b'conv2_block2_1_bn/moving_mean:0' b'conv2_block2_1_bn/moving_variance:0']\n",
            "conv2_block2_1_bn/conv2_block2_1_bn\n",
            "conv2_block2_1_bn/conv2_block2_1_bn/beta:0\n",
            "conv2_block2_1_bn/conv2_block2_1_bn/gamma:0\n",
            "conv2_block2_1_bn/conv2_block2_1_bn/moving_mean:0\n",
            "conv2_block2_1_bn/conv2_block2_1_bn/moving_variance:0\n",
            "conv2_block2_1_conv\n",
            "    weight_names: [b'conv2_block2_1_conv/kernel:0' b'conv2_block2_1_conv/bias:0']\n",
            "conv2_block2_1_conv/conv2_block2_1_conv\n",
            "conv2_block2_1_conv/conv2_block2_1_conv/bias:0\n",
            "conv2_block2_1_conv/conv2_block2_1_conv/kernel:0\n",
            "conv2_block2_1_relu\n",
            "    weight_names: []\n",
            "conv2_block2_2_bn\n",
            "    weight_names: [b'conv2_block2_2_bn/gamma:0' b'conv2_block2_2_bn/beta:0'\n",
            " b'conv2_block2_2_bn/moving_mean:0' b'conv2_block2_2_bn/moving_variance:0']\n",
            "conv2_block2_2_bn/conv2_block2_2_bn\n",
            "conv2_block2_2_bn/conv2_block2_2_bn/beta:0\n",
            "conv2_block2_2_bn/conv2_block2_2_bn/gamma:0\n",
            "conv2_block2_2_bn/conv2_block2_2_bn/moving_mean:0\n",
            "conv2_block2_2_bn/conv2_block2_2_bn/moving_variance:0\n",
            "conv2_block2_2_conv\n",
            "    weight_names: [b'conv2_block2_2_conv/kernel:0' b'conv2_block2_2_conv/bias:0']\n",
            "conv2_block2_2_conv/conv2_block2_2_conv\n",
            "conv2_block2_2_conv/conv2_block2_2_conv/bias:0\n",
            "conv2_block2_2_conv/conv2_block2_2_conv/kernel:0\n",
            "conv2_block2_2_relu\n",
            "    weight_names: []\n",
            "conv2_block2_3_bn\n",
            "    weight_names: [b'conv2_block2_3_bn/gamma:0' b'conv2_block2_3_bn/beta:0'\n",
            " b'conv2_block2_3_bn/moving_mean:0' b'conv2_block2_3_bn/moving_variance:0']\n",
            "conv2_block2_3_bn/conv2_block2_3_bn\n",
            "conv2_block2_3_bn/conv2_block2_3_bn/beta:0\n",
            "conv2_block2_3_bn/conv2_block2_3_bn/gamma:0\n",
            "conv2_block2_3_bn/conv2_block2_3_bn/moving_mean:0\n",
            "conv2_block2_3_bn/conv2_block2_3_bn/moving_variance:0\n",
            "conv2_block2_3_conv\n",
            "    weight_names: [b'conv2_block2_3_conv/kernel:0' b'conv2_block2_3_conv/bias:0']\n",
            "conv2_block2_3_conv/conv2_block2_3_conv\n",
            "conv2_block2_3_conv/conv2_block2_3_conv/bias:0\n",
            "conv2_block2_3_conv/conv2_block2_3_conv/kernel:0\n",
            "conv2_block2_add\n",
            "    weight_names: []\n",
            "conv2_block2_out\n",
            "    weight_names: []\n",
            "conv2_block3_1_bn\n",
            "    weight_names: [b'conv2_block3_1_bn/gamma:0' b'conv2_block3_1_bn/beta:0'\n",
            " b'conv2_block3_1_bn/moving_mean:0' b'conv2_block3_1_bn/moving_variance:0']\n",
            "conv2_block3_1_bn/conv2_block3_1_bn\n",
            "conv2_block3_1_bn/conv2_block3_1_bn/beta:0\n",
            "conv2_block3_1_bn/conv2_block3_1_bn/gamma:0\n",
            "conv2_block3_1_bn/conv2_block3_1_bn/moving_mean:0\n",
            "conv2_block3_1_bn/conv2_block3_1_bn/moving_variance:0\n",
            "conv2_block3_1_conv\n",
            "    weight_names: [b'conv2_block3_1_conv/kernel:0' b'conv2_block3_1_conv/bias:0']\n",
            "conv2_block3_1_conv/conv2_block3_1_conv\n",
            "conv2_block3_1_conv/conv2_block3_1_conv/bias:0\n",
            "conv2_block3_1_conv/conv2_block3_1_conv/kernel:0\n",
            "conv2_block3_1_relu\n",
            "    weight_names: []\n",
            "conv2_block3_2_bn\n",
            "    weight_names: [b'conv2_block3_2_bn/gamma:0' b'conv2_block3_2_bn/beta:0'\n",
            " b'conv2_block3_2_bn/moving_mean:0' b'conv2_block3_2_bn/moving_variance:0']\n",
            "conv2_block3_2_bn/conv2_block3_2_bn\n",
            "conv2_block3_2_bn/conv2_block3_2_bn/beta:0\n",
            "conv2_block3_2_bn/conv2_block3_2_bn/gamma:0\n",
            "conv2_block3_2_bn/conv2_block3_2_bn/moving_mean:0\n",
            "conv2_block3_2_bn/conv2_block3_2_bn/moving_variance:0\n",
            "conv2_block3_2_conv\n",
            "    weight_names: [b'conv2_block3_2_conv/kernel:0' b'conv2_block3_2_conv/bias:0']\n",
            "conv2_block3_2_conv/conv2_block3_2_conv\n",
            "conv2_block3_2_conv/conv2_block3_2_conv/bias:0\n",
            "conv2_block3_2_conv/conv2_block3_2_conv/kernel:0\n",
            "conv2_block3_2_relu\n",
            "    weight_names: []\n",
            "conv2_block3_3_bn\n",
            "    weight_names: [b'conv2_block3_3_bn/gamma:0' b'conv2_block3_3_bn/beta:0'\n",
            " b'conv2_block3_3_bn/moving_mean:0' b'conv2_block3_3_bn/moving_variance:0']\n",
            "conv2_block3_3_bn/conv2_block3_3_bn\n",
            "conv2_block3_3_bn/conv2_block3_3_bn/beta:0\n",
            "conv2_block3_3_bn/conv2_block3_3_bn/gamma:0\n",
            "conv2_block3_3_bn/conv2_block3_3_bn/moving_mean:0\n",
            "conv2_block3_3_bn/conv2_block3_3_bn/moving_variance:0\n",
            "conv2_block3_3_conv\n",
            "    weight_names: [b'conv2_block3_3_conv/kernel:0' b'conv2_block3_3_conv/bias:0']\n",
            "conv2_block3_3_conv/conv2_block3_3_conv\n",
            "conv2_block3_3_conv/conv2_block3_3_conv/bias:0\n",
            "conv2_block3_3_conv/conv2_block3_3_conv/kernel:0\n",
            "conv2_block3_add\n",
            "    weight_names: []\n",
            "conv2_block3_out\n",
            "    weight_names: []\n",
            "conv2d\n",
            "    weight_names: [b'conv2d/kernel:0' b'conv2d/bias:0']\n",
            "conv2d/conv2d\n",
            "conv2d/conv2d/bias:0\n",
            "conv2d/conv2d/kernel:0\n",
            "conv2d_1\n",
            "    weight_names: [b'conv2d_1/kernel:0']\n",
            "conv2d_1/conv2d_1\n",
            "conv2d_1/conv2d_1/kernel:0\n",
            "conv2d_2\n",
            "    weight_names: [b'conv2d_2/kernel:0']\n",
            "conv2d_2/conv2d_2\n",
            "conv2d_2/conv2d_2/kernel:0\n",
            "conv2d_3\n",
            "    weight_names: [b'conv2d_3/kernel:0']\n",
            "conv2d_3/conv2d_3\n",
            "conv2d_3/conv2d_3/kernel:0\n",
            "conv2d_4\n",
            "    weight_names: [b'conv2d_4/kernel:0']\n",
            "conv2d_4/conv2d_4\n",
            "conv2d_4/conv2d_4/kernel:0\n",
            "conv2d_5\n",
            "    weight_names: [b'conv2d_5/kernel:0']\n",
            "conv2d_5/conv2d_5\n",
            "conv2d_5/conv2d_5/kernel:0\n",
            "conv2d_6\n",
            "    weight_names: [b'conv2d_6/kernel:0']\n",
            "conv2d_6/conv2d_6\n",
            "conv2d_6/conv2d_6/kernel:0\n",
            "conv2d_7\n",
            "    weight_names: [b'conv2d_7/kernel:0']\n",
            "conv2d_7/conv2d_7\n",
            "conv2d_7/conv2d_7/kernel:0\n",
            "conv2d_8\n",
            "    weight_names: [b'conv2d_8/kernel:0']\n",
            "conv2d_8/conv2d_8\n",
            "conv2d_8/conv2d_8/kernel:0\n",
            "conv2d_9\n",
            "    weight_names: [b'conv2d_9/kernel:0' b'conv2d_9/bias:0']\n",
            "conv2d_9/conv2d_9\n",
            "conv2d_9/conv2d_9/bias:0\n",
            "conv2d_9/conv2d_9/kernel:0\n",
            "conv3_block1_0_bn\n",
            "    weight_names: [b'conv3_block1_0_bn/gamma:0' b'conv3_block1_0_bn/beta:0'\n",
            " b'conv3_block1_0_bn/moving_mean:0' b'conv3_block1_0_bn/moving_variance:0']\n",
            "conv3_block1_0_bn/conv3_block1_0_bn\n",
            "conv3_block1_0_bn/conv3_block1_0_bn/beta:0\n",
            "conv3_block1_0_bn/conv3_block1_0_bn/gamma:0\n",
            "conv3_block1_0_bn/conv3_block1_0_bn/moving_mean:0\n",
            "conv3_block1_0_bn/conv3_block1_0_bn/moving_variance:0\n",
            "conv3_block1_0_conv\n",
            "    weight_names: [b'conv3_block1_0_conv/kernel:0' b'conv3_block1_0_conv/bias:0']\n",
            "conv3_block1_0_conv/conv3_block1_0_conv\n",
            "conv3_block1_0_conv/conv3_block1_0_conv/bias:0\n",
            "conv3_block1_0_conv/conv3_block1_0_conv/kernel:0\n",
            "conv3_block1_1_bn\n",
            "    weight_names: [b'conv3_block1_1_bn/gamma:0' b'conv3_block1_1_bn/beta:0'\n",
            " b'conv3_block1_1_bn/moving_mean:0' b'conv3_block1_1_bn/moving_variance:0']\n",
            "conv3_block1_1_bn/conv3_block1_1_bn\n",
            "conv3_block1_1_bn/conv3_block1_1_bn/beta:0\n",
            "conv3_block1_1_bn/conv3_block1_1_bn/gamma:0\n",
            "conv3_block1_1_bn/conv3_block1_1_bn/moving_mean:0\n",
            "conv3_block1_1_bn/conv3_block1_1_bn/moving_variance:0\n",
            "conv3_block1_1_conv\n",
            "    weight_names: [b'conv3_block1_1_conv/kernel:0' b'conv3_block1_1_conv/bias:0']\n",
            "conv3_block1_1_conv/conv3_block1_1_conv\n",
            "conv3_block1_1_conv/conv3_block1_1_conv/bias:0\n",
            "conv3_block1_1_conv/conv3_block1_1_conv/kernel:0\n",
            "conv3_block1_1_relu\n",
            "    weight_names: []\n",
            "conv3_block1_2_bn\n",
            "    weight_names: [b'conv3_block1_2_bn/gamma:0' b'conv3_block1_2_bn/beta:0'\n",
            " b'conv3_block1_2_bn/moving_mean:0' b'conv3_block1_2_bn/moving_variance:0']\n",
            "conv3_block1_2_bn/conv3_block1_2_bn\n",
            "conv3_block1_2_bn/conv3_block1_2_bn/beta:0\n",
            "conv3_block1_2_bn/conv3_block1_2_bn/gamma:0\n",
            "conv3_block1_2_bn/conv3_block1_2_bn/moving_mean:0\n",
            "conv3_block1_2_bn/conv3_block1_2_bn/moving_variance:0\n",
            "conv3_block1_2_conv\n",
            "    weight_names: [b'conv3_block1_2_conv/kernel:0' b'conv3_block1_2_conv/bias:0']\n",
            "conv3_block1_2_conv/conv3_block1_2_conv\n",
            "conv3_block1_2_conv/conv3_block1_2_conv/bias:0\n",
            "conv3_block1_2_conv/conv3_block1_2_conv/kernel:0\n",
            "conv3_block1_2_relu\n",
            "    weight_names: []\n",
            "conv3_block1_3_bn\n",
            "    weight_names: [b'conv3_block1_3_bn/gamma:0' b'conv3_block1_3_bn/beta:0'\n",
            " b'conv3_block1_3_bn/moving_mean:0' b'conv3_block1_3_bn/moving_variance:0']\n",
            "conv3_block1_3_bn/conv3_block1_3_bn\n",
            "conv3_block1_3_bn/conv3_block1_3_bn/beta:0\n",
            "conv3_block1_3_bn/conv3_block1_3_bn/gamma:0\n",
            "conv3_block1_3_bn/conv3_block1_3_bn/moving_mean:0\n",
            "conv3_block1_3_bn/conv3_block1_3_bn/moving_variance:0\n",
            "conv3_block1_3_conv\n",
            "    weight_names: [b'conv3_block1_3_conv/kernel:0' b'conv3_block1_3_conv/bias:0']\n",
            "conv3_block1_3_conv/conv3_block1_3_conv\n",
            "conv3_block1_3_conv/conv3_block1_3_conv/bias:0\n",
            "conv3_block1_3_conv/conv3_block1_3_conv/kernel:0\n",
            "conv3_block1_add\n",
            "    weight_names: []\n",
            "conv3_block1_out\n",
            "    weight_names: []\n",
            "conv3_block2_1_bn\n",
            "    weight_names: [b'conv3_block2_1_bn/gamma:0' b'conv3_block2_1_bn/beta:0'\n",
            " b'conv3_block2_1_bn/moving_mean:0' b'conv3_block2_1_bn/moving_variance:0']\n",
            "conv3_block2_1_bn/conv3_block2_1_bn\n",
            "conv3_block2_1_bn/conv3_block2_1_bn/beta:0\n",
            "conv3_block2_1_bn/conv3_block2_1_bn/gamma:0\n",
            "conv3_block2_1_bn/conv3_block2_1_bn/moving_mean:0\n",
            "conv3_block2_1_bn/conv3_block2_1_bn/moving_variance:0\n",
            "conv3_block2_1_conv\n",
            "    weight_names: [b'conv3_block2_1_conv/kernel:0' b'conv3_block2_1_conv/bias:0']\n",
            "conv3_block2_1_conv/conv3_block2_1_conv\n",
            "conv3_block2_1_conv/conv3_block2_1_conv/bias:0\n",
            "conv3_block2_1_conv/conv3_block2_1_conv/kernel:0\n",
            "conv3_block2_1_relu\n",
            "    weight_names: []\n",
            "conv3_block2_2_bn\n",
            "    weight_names: [b'conv3_block2_2_bn/gamma:0' b'conv3_block2_2_bn/beta:0'\n",
            " b'conv3_block2_2_bn/moving_mean:0' b'conv3_block2_2_bn/moving_variance:0']\n",
            "conv3_block2_2_bn/conv3_block2_2_bn\n",
            "conv3_block2_2_bn/conv3_block2_2_bn/beta:0\n",
            "conv3_block2_2_bn/conv3_block2_2_bn/gamma:0\n",
            "conv3_block2_2_bn/conv3_block2_2_bn/moving_mean:0\n",
            "conv3_block2_2_bn/conv3_block2_2_bn/moving_variance:0\n",
            "conv3_block2_2_conv\n",
            "    weight_names: [b'conv3_block2_2_conv/kernel:0' b'conv3_block2_2_conv/bias:0']\n",
            "conv3_block2_2_conv/conv3_block2_2_conv\n",
            "conv3_block2_2_conv/conv3_block2_2_conv/bias:0\n",
            "conv3_block2_2_conv/conv3_block2_2_conv/kernel:0\n",
            "conv3_block2_2_relu\n",
            "    weight_names: []\n",
            "conv3_block2_3_bn\n",
            "    weight_names: [b'conv3_block2_3_bn/gamma:0' b'conv3_block2_3_bn/beta:0'\n",
            " b'conv3_block2_3_bn/moving_mean:0' b'conv3_block2_3_bn/moving_variance:0']\n",
            "conv3_block2_3_bn/conv3_block2_3_bn\n",
            "conv3_block2_3_bn/conv3_block2_3_bn/beta:0\n",
            "conv3_block2_3_bn/conv3_block2_3_bn/gamma:0\n",
            "conv3_block2_3_bn/conv3_block2_3_bn/moving_mean:0\n",
            "conv3_block2_3_bn/conv3_block2_3_bn/moving_variance:0\n",
            "conv3_block2_3_conv\n",
            "    weight_names: [b'conv3_block2_3_conv/kernel:0' b'conv3_block2_3_conv/bias:0']\n",
            "conv3_block2_3_conv/conv3_block2_3_conv\n",
            "conv3_block2_3_conv/conv3_block2_3_conv/bias:0\n",
            "conv3_block2_3_conv/conv3_block2_3_conv/kernel:0\n",
            "conv3_block2_add\n",
            "    weight_names: []\n",
            "conv3_block2_out\n",
            "    weight_names: []\n",
            "conv3_block3_1_bn\n",
            "    weight_names: [b'conv3_block3_1_bn/gamma:0' b'conv3_block3_1_bn/beta:0'\n",
            " b'conv3_block3_1_bn/moving_mean:0' b'conv3_block3_1_bn/moving_variance:0']\n",
            "conv3_block3_1_bn/conv3_block3_1_bn\n",
            "conv3_block3_1_bn/conv3_block3_1_bn/beta:0\n",
            "conv3_block3_1_bn/conv3_block3_1_bn/gamma:0\n",
            "conv3_block3_1_bn/conv3_block3_1_bn/moving_mean:0\n",
            "conv3_block3_1_bn/conv3_block3_1_bn/moving_variance:0\n",
            "conv3_block3_1_conv\n",
            "    weight_names: [b'conv3_block3_1_conv/kernel:0' b'conv3_block3_1_conv/bias:0']\n",
            "conv3_block3_1_conv/conv3_block3_1_conv\n",
            "conv3_block3_1_conv/conv3_block3_1_conv/bias:0\n",
            "conv3_block3_1_conv/conv3_block3_1_conv/kernel:0\n",
            "conv3_block3_1_relu\n",
            "    weight_names: []\n",
            "conv3_block3_2_bn\n",
            "    weight_names: [b'conv3_block3_2_bn/gamma:0' b'conv3_block3_2_bn/beta:0'\n",
            " b'conv3_block3_2_bn/moving_mean:0' b'conv3_block3_2_bn/moving_variance:0']\n",
            "conv3_block3_2_bn/conv3_block3_2_bn\n",
            "conv3_block3_2_bn/conv3_block3_2_bn/beta:0\n",
            "conv3_block3_2_bn/conv3_block3_2_bn/gamma:0\n",
            "conv3_block3_2_bn/conv3_block3_2_bn/moving_mean:0\n",
            "conv3_block3_2_bn/conv3_block3_2_bn/moving_variance:0\n",
            "conv3_block3_2_conv\n",
            "    weight_names: [b'conv3_block3_2_conv/kernel:0' b'conv3_block3_2_conv/bias:0']\n",
            "conv3_block3_2_conv/conv3_block3_2_conv\n",
            "conv3_block3_2_conv/conv3_block3_2_conv/bias:0\n",
            "conv3_block3_2_conv/conv3_block3_2_conv/kernel:0\n",
            "conv3_block3_2_relu\n",
            "    weight_names: []\n",
            "input_1\n",
            "    weight_names: []\n",
            "pool1_pad\n",
            "    weight_names: []\n",
            "pool1_pool\n",
            "    weight_names: []\n",
            "tf.nn.relu\n",
            "    weight_names: []\n",
            "tf.nn.relu_1\n",
            "    weight_names: []\n",
            "tf.nn.relu_2\n",
            "    weight_names: []\n",
            "tf.nn.relu_3\n",
            "    weight_names: []\n",
            "tf.nn.relu_4\n",
            "    weight_names: []\n",
            "tf.nn.relu_5\n",
            "    weight_names: []\n",
            "tf.nn.relu_6\n",
            "    weight_names: []\n",
            "tf.nn.relu_7\n",
            "    weight_names: []\n",
            "tf.nn.relu_8\n",
            "    weight_names: []\n",
            "top_level_model_weights\n",
            "    weight_names: []\n",
            "up_sampling2d\n",
            "    weight_names: []\n",
            "up_sampling2d_1\n",
            "    weight_names: []\n",
            "up_sampling2d_2\n",
            "    weight_names: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def explore_hdf5_group(group, indent=0):\n",
        "    for key, value in group.items():\n",
        "        print(\"  \" * indent + f\"Key: {key}\")\n",
        "        if isinstance(value, h5py.Group):\n",
        "            explore_hdf5_group(value, indent + 1)\n",
        "        elif isinstance(value, h5py.Dataset):\n",
        "            print(\"  \" * (indent + 1) + f\"Dataset shape: {value.shape}, type: {value.dtype}\")\n",
        "\n",
        "def explore_hdf5_file(file_path):\n",
        "    with h5py.File(file_path, 'r') as file:\n",
        "        print(\"Root level keys and attributes:\")\n",
        "        explore_hdf5_group(file)\n",
        "\n",
        "file_path = 'SegModels/segmenter.06-0.04.hdf5'\n",
        "explore_hdf5_file(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZhiisSRWV4M",
        "outputId": "3d71fca5-8e82-48cb-a1ff-6dd61b3877a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root level keys and attributes:\n",
            "Key: average_pooling2d\n",
            "Key: batch_normalization\n",
            "  Key: batch_normalization\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_1\n",
            "  Key: batch_normalization_1\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_2\n",
            "  Key: batch_normalization_2\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_3\n",
            "  Key: batch_normalization_3\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_4\n",
            "  Key: batch_normalization_4\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_5\n",
            "  Key: batch_normalization_5\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_6\n",
            "  Key: batch_normalization_6\n",
            "    Key: beta:0\n",
            "      Dataset shape: (48,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (48,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (48,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (48,), type: float32\n",
            "Key: batch_normalization_7\n",
            "  Key: batch_normalization_7\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: batch_normalization_8\n",
            "  Key: batch_normalization_8\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: concatenate\n",
            "Key: concatenate_1\n",
            "Key: conv1_bn\n",
            "  Key: conv1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv1_conv\n",
            "  Key: conv1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (7, 7, 3, 64), type: float32\n",
            "Key: conv1_pad\n",
            "Key: conv1_relu\n",
            "Key: conv2_block1_0_bn\n",
            "  Key: conv2_block1_0_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: conv2_block1_0_conv\n",
            "  Key: conv2_block1_0_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 256), type: float32\n",
            "Key: conv2_block1_1_bn\n",
            "  Key: conv2_block1_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block1_1_conv\n",
            "  Key: conv2_block1_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 64), type: float32\n",
            "Key: conv2_block1_1_relu\n",
            "Key: conv2_block1_2_bn\n",
            "  Key: conv2_block1_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block1_2_conv\n",
            "  Key: conv2_block1_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 64, 64), type: float32\n",
            "Key: conv2_block1_2_relu\n",
            "Key: conv2_block1_3_bn\n",
            "  Key: conv2_block1_3_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: conv2_block1_3_conv\n",
            "  Key: conv2_block1_3_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 256), type: float32\n",
            "Key: conv2_block1_add\n",
            "Key: conv2_block1_out\n",
            "Key: conv2_block2_1_bn\n",
            "  Key: conv2_block2_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block2_1_conv\n",
            "  Key: conv2_block2_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 256, 64), type: float32\n",
            "Key: conv2_block2_1_relu\n",
            "Key: conv2_block2_2_bn\n",
            "  Key: conv2_block2_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block2_2_conv\n",
            "  Key: conv2_block2_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 64, 64), type: float32\n",
            "Key: conv2_block2_2_relu\n",
            "Key: conv2_block2_3_bn\n",
            "  Key: conv2_block2_3_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: conv2_block2_3_conv\n",
            "  Key: conv2_block2_3_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 256), type: float32\n",
            "Key: conv2_block2_add\n",
            "Key: conv2_block2_out\n",
            "Key: conv2_block3_1_bn\n",
            "  Key: conv2_block3_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block3_1_conv\n",
            "  Key: conv2_block3_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 256, 64), type: float32\n",
            "Key: conv2_block3_1_relu\n",
            "Key: conv2_block3_2_bn\n",
            "  Key: conv2_block3_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "Key: conv2_block3_2_conv\n",
            "  Key: conv2_block3_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (64,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 64, 64), type: float32\n",
            "Key: conv2_block3_2_relu\n",
            "Key: conv2_block3_3_bn\n",
            "  Key: conv2_block3_3_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "Key: conv2_block3_3_conv\n",
            "  Key: conv2_block3_3_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 256), type: float32\n",
            "Key: conv2_block3_add\n",
            "Key: conv2_block3_out\n",
            "Key: conv2d\n",
            "  Key: conv2d\n",
            "    Key: bias:0\n",
            "      Dataset shape: (256,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 128, 256), type: float32\n",
            "Key: conv2d_1\n",
            "  Key: conv2d_1\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 128, 256), type: float32\n",
            "Key: conv2d_2\n",
            "  Key: conv2d_2\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 256), type: float32\n",
            "Key: conv2d_3\n",
            "  Key: conv2d_3\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 256), type: float32\n",
            "Key: conv2d_4\n",
            "  Key: conv2d_4\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 256), type: float32\n",
            "Key: conv2d_5\n",
            "  Key: conv2d_5\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 1280, 256), type: float32\n",
            "Key: conv2d_6\n",
            "  Key: conv2d_6\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 64, 48), type: float32\n",
            "Key: conv2d_7\n",
            "  Key: conv2d_7\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 304, 256), type: float32\n",
            "Key: conv2d_8\n",
            "  Key: conv2d_8\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 256, 256), type: float32\n",
            "Key: conv2d_9\n",
            "  Key: conv2d_9\n",
            "    Key: bias:0\n",
            "      Dataset shape: (2,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 256, 2), type: float32\n",
            "Key: conv3_block1_0_bn\n",
            "  Key: conv3_block1_0_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "Key: conv3_block1_0_conv\n",
            "  Key: conv3_block1_0_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 256, 512), type: float32\n",
            "Key: conv3_block1_1_bn\n",
            "  Key: conv3_block1_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block1_1_conv\n",
            "  Key: conv3_block1_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 256, 128), type: float32\n",
            "Key: conv3_block1_1_relu\n",
            "Key: conv3_block1_2_bn\n",
            "  Key: conv3_block1_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block1_2_conv\n",
            "  Key: conv3_block1_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 128), type: float32\n",
            "Key: conv3_block1_2_relu\n",
            "Key: conv3_block1_3_bn\n",
            "  Key: conv3_block1_3_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "Key: conv3_block1_3_conv\n",
            "  Key: conv3_block1_3_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 128, 512), type: float32\n",
            "Key: conv3_block1_add\n",
            "Key: conv3_block1_out\n",
            "Key: conv3_block2_1_bn\n",
            "  Key: conv3_block2_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block2_1_conv\n",
            "  Key: conv3_block2_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 512, 128), type: float32\n",
            "Key: conv3_block2_1_relu\n",
            "Key: conv3_block2_2_bn\n",
            "  Key: conv3_block2_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block2_2_conv\n",
            "  Key: conv3_block2_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 128), type: float32\n",
            "Key: conv3_block2_2_relu\n",
            "Key: conv3_block2_3_bn\n",
            "  Key: conv3_block2_3_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "Key: conv3_block2_3_conv\n",
            "  Key: conv3_block2_3_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (512,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 128, 512), type: float32\n",
            "Key: conv3_block2_add\n",
            "Key: conv3_block2_out\n",
            "Key: conv3_block3_1_bn\n",
            "  Key: conv3_block3_1_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block3_1_conv\n",
            "  Key: conv3_block3_1_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (1, 1, 512, 128), type: float32\n",
            "Key: conv3_block3_1_relu\n",
            "Key: conv3_block3_2_bn\n",
            "  Key: conv3_block3_2_bn\n",
            "    Key: beta:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: gamma:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_mean:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: moving_variance:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "Key: conv3_block3_2_conv\n",
            "  Key: conv3_block3_2_conv\n",
            "    Key: bias:0\n",
            "      Dataset shape: (128,), type: float32\n",
            "    Key: kernel:0\n",
            "      Dataset shape: (3, 3, 128, 128), type: float32\n",
            "Key: conv3_block3_2_relu\n",
            "Key: input_1\n",
            "Key: pool1_pad\n",
            "Key: pool1_pool\n",
            "Key: tf.nn.relu\n",
            "Key: tf.nn.relu_1\n",
            "Key: tf.nn.relu_2\n",
            "Key: tf.nn.relu_3\n",
            "Key: tf.nn.relu_4\n",
            "Key: tf.nn.relu_5\n",
            "Key: tf.nn.relu_6\n",
            "Key: tf.nn.relu_7\n",
            "Key: tf.nn.relu_8\n",
            "Key: top_level_model_weights\n",
            "Key: up_sampling2d\n",
            "Key: up_sampling2d_1\n",
            "Key: up_sampling2d_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "vpY9aG6qi5qi",
        "outputId": "dbe3fe6c-9080-4efd-fb26-36eb690357ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbe198b8340>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = train_gen.__getitem__(0)\n",
        "p = network_model.predict(image[0])\n",
        "#plt.imshow(np.argmax(p[0],axis=2))\n",
        "plt.imshow((image[1][0]*5)+np.argmax(p[0],axis=2,keepdims=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input, i):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    dilation_rate = i # 2 to 12 possibility\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate*2)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=dilation_rate*3)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n",
        "def DeeplabV3Plus1(i):\n",
        "    image_size = 224\n",
        "    num_classes = 2\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = tf.keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    #trainable_layers = hp.Int(\"trainable_layers\",min_value=0,max_value=len(resnet50.layers))\n",
        "    for idx,layer in enumerate(reversed(resnet50.layers)):\n",
        "        #if idx == trainable_layers:\n",
        "            #break\n",
        "        layer.trainable = True\n",
        "    x = resnet50.get_layer(\"conv3_block3_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x, i)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    model = keras.Model(inputs=model_input, outputs=model_output)\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-8),metrics=[\"accuracy\",dice_coef])\n",
        "    return model"
      ],
      "metadata": {
        "id": "oj-WVaXwZ-R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = PrepareImagesGen(metadata.loc[metadata[\"Split\"] == \"TRAIN\",:],batch_size=batch_size)"
      ],
      "metadata": {
        "id": "MFeIZm9lee3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [train_gen.__getitem__(i) for i in range(4)]"
      ],
      "metadata": {
        "id": "3kYJlUUKgiIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "c94e3cd6-abce-4925-9ebc-0a3ac12163c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a26c197cf92c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-a26c197cf92c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-4ff9f600048a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e1e2560d3a29>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(df, root_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtemp_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtemp_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myield_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtemp_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtemp_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-47f7a49e30b5>\u001b[0m in \u001b[0;36myield_segmentation\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mseg2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mseg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_avi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a11be3bb92b2>\u001b[0m in \u001b[0;36mload_avi\u001b[0;34m(path, max_frames)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_avi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcapture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Videos/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".avi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,7):\n",
        "  model = DeeplabV3Plus1(i)\n",
        "  model.load_weights('SegModels/segmenter.06-0.04.hdf5')\n",
        "  j = 0\n",
        "  print(i)\n",
        "  for image in images:\n",
        "    print(j, model.evaluate(x=image[0],y=image[1]),sep='\\n')\n",
        "    j += 1\n",
        "  print('------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6ZqjnbddCvA",
        "outputId": "4191b9f3-6570-49cd-9240-db9844924813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "(32, 50176) (32, 50176)\n",
            "(32, 50176) (32, 50176)\n",
            "4/4 [==============================] - 9s 27ms/step - loss: 0.0404 - accuracy: 0.9834 - dice_coef: 0.8936\n",
            "0\n",
            "[0.04040432721376419, 0.9833646416664124, 0.8936485052108765]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0428 - accuracy: 0.9823 - dice_coef: 0.8963\n",
            "1\n",
            "[0.04284578189253807, 0.9823237657546997, 0.8963123559951782]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0444 - accuracy: 0.9819 - dice_coef: 0.8753\n",
            "2\n",
            "[0.044398702681064606, 0.9818532466888428, 0.8752582669258118]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0395 - accuracy: 0.9837 - dice_coef: 0.8958\n",
            "3\n",
            "[0.039512261748313904, 0.9837148189544678, 0.8958278298377991]\n",
            "------------------\n",
            "(32, 50176) (32, 50176)\n",
            "(32, 50176) (32, 50176)\n",
            "4/4 [==============================] - 1s 26ms/step - loss: 0.0288 - accuracy: 0.9875 - dice_coef: 0.9120\n",
            "0\n",
            "[0.028762757778167725, 0.9875261187553406, 0.9120411276817322]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0306 - accuracy: 0.9865 - dice_coef: 0.9140\n",
            "1\n",
            "[0.030644942075014114, 0.9864631295204163, 0.9140432476997375]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0343 - accuracy: 0.9855 - dice_coef: 0.8905\n",
            "2\n",
            "[0.03425571694970131, 0.9855417013168335, 0.8905088901519775]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0296 - accuracy: 0.9870 - dice_coef: 0.9116\n",
            "3\n",
            "[0.029620053246617317, 0.987016499042511, 0.9116038680076599]\n",
            "------------------\n",
            "(32, 50176) (32, 50176)\n",
            "(32, 50176) (32, 50176)\n",
            "4/4 [==============================] - 1s 25ms/step - loss: 0.0326 - accuracy: 0.9857 - dice_coef: 0.8945\n",
            "0\n",
            "[0.03260321170091629, 0.9857063889503479, 0.8945070505142212]\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.0354 - accuracy: 0.9847 - dice_coef: 0.9006\n",
            "1\n",
            "[0.03541938215494156, 0.9847144484519958, 0.900592565536499]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0389 - accuracy: 0.9836 - dice_coef: 0.8695\n",
            "2\n",
            "[0.03894774988293648, 0.9836367964744568, 0.8694634437561035]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0343 - accuracy: 0.9849 - dice_coef: 0.8949\n",
            "3\n",
            "[0.03434218093752861, 0.9849302172660828, 0.8948980569839478]\n",
            "------------------\n",
            "(32, 50176) (32, 50176)\n",
            "(32, 50176) (32, 50176)\n",
            "4/4 [==============================] - 1s 26ms/step - loss: 0.0469 - accuracy: 0.9801 - dice_coef: 0.8353\n",
            "0\n",
            "[0.04687989503145218, 0.9800662398338318, 0.8353142142295837]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0518 - accuracy: 0.9776 - dice_coef: 0.8322\n",
            "1\n",
            "[0.05181717127561569, 0.9776049256324768, 0.8321816325187683]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0522 - accuracy: 0.9780 - dice_coef: 0.8099\n",
            "2\n",
            "[0.052193351089954376, 0.9779887199401855, 0.8098553419113159]\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0483 - accuracy: 0.9789 - dice_coef: 0.8354\n",
            "3\n",
            "[0.048302359879016876, 0.9789101481437683, 0.835415244102478]\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "def save_overlay_as_pdf(image, segmentation, filename):\n",
        "    # Create a binary mask from the segmentation\n",
        "    binary_mask = np.argmax(segmentation, axis=-1)\n",
        "\n",
        "    # Create a color mask with the segmentation in blue, normalized\n",
        "    color_mask = np.zeros_like(image)\n",
        "    color_mask[binary_mask == 1] = [0, 0, 1] # Blue color, normalized\n",
        "\n",
        "    # Overlay the color mask on the original image with 75% opacity\n",
        "    overlayed_image = cv2.addWeighted(image, 0.25, color_mask, 0.75, 0) # 75% opacity\n",
        "\n",
        "    # Plot and save as PDF\n",
        "    #plt.tight_layout()\n",
        "    plt.imshow(overlayed_image)\n",
        "    plt.axis('off') # Remove axis\n",
        "    #plt.tight_layout()\n",
        "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.0, format='pdf')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have a test generator or some images loaded\n",
        "test_gen = PrepareImagesGen(metadata.loc[metadata[\"Split\"] == \"TEST\",:], batch_size=10)\n",
        "\n",
        "# Load the model\n",
        "model = DeeplabV3Plus1(4)\n",
        "model.load_weights('SegModels/segmenter.06-0.04.hdf5')\n",
        "\n",
        "# Get a batch of images and segmentations\n",
        "images, _ = test_gen.__getitem__(0)\n",
        "\n",
        "# Predict segmentations\n",
        "segmentations = model.predict(images)\n",
        "\n",
        "# Overlay segmentations and save as PDFs\n",
        "for i in tqdm(range(10)):\n",
        "    save_overlay_as_pdf(images[i], segmentations[i], f'segmentation_{i}.pdf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMIC9t6tRtHJ",
        "outputId": "0661088f-9a82-4e10-ffd0-0141c72706d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 549ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.34it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}