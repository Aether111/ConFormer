# -*- coding: utf-8 -*-
"""Copy of Copy_of_Preprocessor (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GmRqifCBZfP-adjkZB9L2JMFfX3M-IOZ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
from tqdm import tqdm
import glob
import keras
from keras.models import *
from keras.layers import *
from keras import layers
import tensorflow as tf
import os

# %matplotlib inline

file_list = set()
for batch in ["/Batch1/", "/Batch2/", "/Batch3/", "/Batch4/"]:
  os.chdir(os.getcwd()+batch)
  file_list = file_list.union(set(glob.glob("*.avi")))
  os.chdir("..")

data = pd.read_csv("MeasurementsList.csv")
data.drop(columns=["Unnamed: 0"], inplace=True)
mask = (data["Calc"] == "LVPWd") | (data["Calc"] == "LVIDd") | (data["Calc"] == "IVSd")
data = data[mask]

data["HashedFileNameAVI"] = data["HashedFileName"] + ".avi"
df_list = set(data["HashedFileNameAVI"].unique())
to_remove = df_list.difference(file_list)
data = data[~data["HashedFileNameAVI"].isin(to_remove)]

MAX_SEQ_LENGTH = 32
IMAGE_DIMS = (640, 480) # (224, 224)
BATCH_SIZE = 10
IMAGE_CHANNELS = 3
IMAGE_OUT_DIMS = 4
learning_rate = 1e-3

def crop_and_scale(img, res=(640, 480)):
    """Scales and cropts an numpy array image to specified resolution.
    Image is first cropped to correct aspect ratio and then scaled using
    bicubic interpolation.
    Args:
        img (np.ndarray): Image to be resized. shape=(h, w, 3)
        res (tuple, optional): Resolution to be scaled to. Defaults to (640, 480).
    Returns:
        np.ndarray: Scaled image. shape=(res[1], res[0], 3)


    in_res = (img.shape[1], img.shape[0])
    r_in = in_res[0] / in_res[1]
    r_out = res[0] / res[1]
    padding = None

    if r_in > r_out:
        padding = int(round((in_res[0] - r_out * in_res[1]) / 2))
        img = img[:, padding:-padding]
    if r_in < r_out:
        padding = int(round((in_res[1] - in_res[0] / r_out) / 2))
        img = img[padding:-padding]
    """

    img = cv2.resize(img, res)

    return img

def load_avi(path, target_frame=0, target_res=IMAGE_DIMS):
  for batch in ["Batch1/", "Batch2/", "Batch3/", "Batch4/"]:
    capture = cv2.VideoCapture(batch + path)
    capture.set(cv2.CAP_PROP_POS_FRAMES, target_frame-1)
    frame = None
    try:
      res, frame = capture.read()
      frame = frame[:,:,[2,1,0]]
      frame = crop_and_scale(frame)
    except Exception as e:
      continue
    break
  capture.release()
  return np.array(frame)/255.

data["HashedFileName"].value_counts()
sub_mask = data["HashedFileName"].value_counts() >= 3
mask = data["HashedFileName"].isin(data["HashedFileName"].value_counts()[sub_mask].index)
clean_data = data[mask]

IVSd  = "IVSd"
LVIDd = "LVIDd"
LVPWd = "LVPWd"

calc_data  = clean_data.groupby(by="HashedFileName")[["Calc"]].value_counts().to_frame().query("Calc == @IVSd or Calc == @LVIDd or Calc == @LVPWd")
calc_mask  = calc_data.groupby(level=[0]).size() == 3
calc_mask  = calc_mask[calc_mask == True]
clean_data = clean_data[clean_data["HashedFileName"].isin(calc_mask.index)]
# Optional to only take diastoles
# data_mask = (clean_data["Calc"] == IVSd) | (clean_data["Calc"] == LVIDd) | (clean_data["Calc"] == LVPWd)
# clean_data = clean_data[data_mask]
clean_data = clean_data[~clean_data.duplicated(["HashedFileName", "Calc"])]
clean_data.reset_index(inplace=True, drop=True)

clean_data["Width"] = clean_data["Width"].astype(int)
clean_data["Height"] = clean_data["Height"].astype(int)
clean_data = clean_data.loc[((clean_data["Width"] == 800) & (clean_data["Height"] == 600)) | ((clean_data["Width"] == 1024) & (clean_data["Height"] == 768)),:]
clean_data["Frames"] = clean_data["Frames"].astype(int)

def run_inference(paths):
  batch = np.zeros((BATCH_SIZE, IMAGE_DIMS[1], IMAGE_DIMS[0], IMAGE_CHANNELS))
  y = np.zeros((BATCH_SIZE, 8))
  for batch_id, path in enumerate(paths):
    target_frame = clean_data.loc[clean_data["HashedFileName"] == path[:-4], "Frame"].iloc[0]
    temp = np.zeros((8))

    for idx, measurement in enumerate([IVSd, LVPWd]):
      mask = (clean_data["HashedFileName"] == path[:-4]) & (clean_data["Calc"] == measurement)
      temp[idx*4:(idx+1)*4] = clean_data.loc[mask, :][["X1", "Y1", "X2", "Y2"]].values.flatten()
      original_dims = clean_data.loc[mask, :][["Width", "Height"]].iloc[0].values
      width, height = original_dims[0], original_dims[1]
      width_d, height_d = IMAGE_DIMS[0] / width, IMAGE_DIMS[1] / height
      temp[(idx*4) + 0] *= width_d
      temp[(idx*4) + 1] *= height_d
      temp[(idx*4) + 2] *= width_d
      temp[(idx*4) + 3] *= height_d
    batch[batch_id] = load_avi(path, target_frame)
    y[batch_id, :]  = temp.copy()

  return batch, y

class PrepareImagesGen(tf.keras.utils.Sequence):
    def __init__(self, df):
        self.df = df.copy()
        self.batch_size = BATCH_SIZE
        self.input_size = IMAGE_DIMS
        self.n = len(self.df["HashedFileName"].unique())

    def on_epoch_end(self):
        pass

    def __getitem__(self,index):
        paths = self.df["HashedFileName"].unique()[index*BATCH_SIZE:((index+1)*BATCH_SIZE)]
        paths = [path + ".avi" for path in paths]
        X,y = run_inference(paths)
        return X,y

    def __len__(self):
        return self.n//self.batch_size

train_gen = PrepareImagesGen(clean_data[clean_data["split"] == "train"])
val_gen = PrepareImagesGen(clean_data[clean_data["split"] == "val"])
test_gen = PrepareImagesGen(clean_data[clean_data["split"] == "test"])

def convolution_block(
    block_input,
    num_filters=256,
    kernel_size=3,
    dilation_rate=1,
    padding="same",
    use_bias=False,
):
    x = layers.Conv2D(
        num_filters,
        kernel_size=kernel_size,
        dilation_rate=dilation_rate,
        padding="same",
        use_bias=use_bias,
        kernel_initializer=keras.initializers.HeNormal()

    )(block_input)
    x = layers.Dropout(0.05)(x)
    x = layers.BatchNormalization()(x)
    return tf.nn.relu(x)


def DilatedSpatialPyramidPooling(dspp_input):
    dims = dspp_input.shape
    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)
    x = convolution_block(x, kernel_size=1, use_bias=True)
    out_pool = layers.UpSampling2D(
        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation="bilinear",
    )(x)

    out_1  = convolution_block(dspp_input, kernel_size=1, dilation_rate=1, padding="same")
    out_6  = convolution_block(dspp_input, kernel_size=3, dilation_rate=3, padding="same")
    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6, padding="same")
    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=9, padding="same")

    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])
    output = convolution_block(x, kernel_size=1)
    return output

# https://github.com/KostasStefanidis/Semantic-Segmentation/blob/2d97dec22f47155ea5b84bc49e42dbeb207d0070/utils/models/SegmentationModels.py#L348

def DeeplabV3Plus(image_size, num_classes):
    height, width = image_size
    model_input = keras.Input(shape=(width, height, 3))
    backbone = tf.keras.applications.EfficientNetV2S(
        weights="imagenet", include_top=False, input_tensor=model_input
    )
    x = backbone.get_layer("top_activation").output
    x = DilatedSpatialPyramidPooling(x)

    input_a = layers.UpSampling2D(
        8,
        interpolation="bilinear",
    )(x)
    input_b = backbone.get_layer("block2d_add").output
    input_b = convolution_block(input_b, num_filters=48, kernel_size=1, use_bias=False)

    x = layers.Concatenate(axis=-1)([input_a, input_b])
    x = convolution_block(x)
    x = convolution_block(x)
    x = layers.UpSampling2D(
        size=(width // x.shape[1], height // x.shape[2]),
        interpolation="bilinear",
    )(x)
    x = layers.Conv2D(num_classes, kernel_size=(1, 1), padding="same")(x) #,kernel_regularizer=keras.regularizers.l2(0.001))(x)
    model_output = tf.nn.sigmoid(x)

    return keras.Model(inputs=model_input, outputs=model_output)

def get_points_np(preds, threshold=0.2):

    """Gets the centroid of heatmaps.
    Args:
        preds (np.ndarray): Input heatmaps. shape=(n, h, w, c)
        threshold (float, optional): Value below which input pixels are ignored. Defaults to 0.3.
    Returns:
        np.ndarray: Centroid locations. shape=(n, c, 2)
    """

    # preds = tf.where(preds < threshold, 0.0, preds) Inference only
    Y, X  = np.mgrid[:IMAGE_DIMS[0], :IMAGE_DIMS[1]]
    X, Y  = tf.convert_to_tensor(X, dtype=tf.float32), tf.convert_to_tensor(Y, dtype=tf.float32)
    x_pts = tf.math.reduce_sum(X[None, ..., None] * preds, axis=(-3, -2)) / (tf.math.reduce_sum(preds, axis=(-3, -2)))
    y_pts = tf.math.reduce_sum(Y[None, ..., None] * preds, axis=(-3, -2)) / (tf.math.reduce_sum(preds, axis=(-3, -2)))

    # Remove nans as a result of division, should be necessary infrequently
    #x_pts = tf.where(tf.math.is_nan(x_pts), tf.zeros_like(x_pts), x_pts)
    #y_pts = tf.where(tf.math.is_nan(y_pts), tf.zeros_like(y_pts), y_pts)
    return tf.transpose(tf.convert_to_tensor([x_pts, y_pts], dtype=tf.float32), perm=[1,2,0])

import keras.backend as K
#https://keras.io/api/metrics/#creating-custom-metrics

def get_lens_np(y_true, y_pred):
    """Used to get the euclidean distance between consecutive points.
    Args:
        pts (np.ndarray): Input points. shape=(..., n, 2)
    Returns:
        np.ndarray: Distances. shape=(..., n-1)
    """
    pts = get_points_np(tf.transpose(y_pred, perm=[0,2,1,3]))
    y_true_pts = tf.reshape(y_true,shape=(BATCH_SIZE,4,2))
    return K.abs((K.sum((y_true_pts[..., 1:, :] - y_true_pts[..., :-1, :]) ** 2, axis=-1) ** 0.5) - (K.sum((pts[..., 1:, :] - pts[..., :-1, :]) ** 2, axis=-1) ** 0.5))

class MUC(tf.keras.losses.Loss):

  def __init__(self):
    super().__init__(reduction=tf.keras.losses.Reduction.AUTO, name="weighted_mean_squared_error")

  def call(self, y_true, y_pred):
    alpha=0.001
    preds = get_points_np(tf.transpose(y_pred,perm=[0,2,1,3]))
    preds = tf.keras.backend.batch_flatten(preds)
    loss = K.mean(alpha*(1-y_true)*(y_true-preds)**2 + (1-alpha)*y_true*(y_true-preds)**2)
    return loss

model = DeeplabV3Plus(IMAGE_DIMS, 4)
model.compile(loss=MUC(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-8), metrics=[get_lens_np]) # possibly include ema and amsgrad
checkpoint = keras.callbacks.ModelCheckpoint(filepath="models/lvhlvh.{epoch:02d}-{val_loss:.2f}.hdf5",save_weights_only=True, save_best_only=True)

# Uncomment to fit
# model.fit(train_gen, batch_size=BATCH_SIZE, shuffle=False, epochs=20, verbose=True, callbacks=[checkpoint], validation_data=val_gen)

def run_inference_on_image(path, target_frame=0):
  frame = load_avi(path, target_frame=target_frame)
  frame = np.expand_dims(frame, axis=0)
  heatmap = model.predict(frame) # Should be 24.6 ms/step
  targets = ["IVS_bottom", "IVS_top", "LVPW_bottom", "LVPW_bottom"]
  fig, axs = plt.subplots(2, 4)
  fig.set_figheight(7)
  fig.set_figwidth(15)
  points = get_points_np(tf.transpose(heatmap, perm=[0,2,1,3]))[0]

  temp = np.zeros((8))
  for idx, measurement in enumerate([IVSd, LVPWd]):
    mask = (clean_data["HashedFileName"] == path[:-4]) & (clean_data["Calc"] == measurement)
    temp[idx*4:(idx+1)*4] = clean_data.loc[mask, :][["X1", "Y1", "X2", "Y2"]].values.flatten()
    original_dims = clean_data.loc[mask, :][["Width", "Height"]].iloc[0].values
    width, height = original_dims[0], original_dims[1]
    width_d, height_d = IMAGE_DIMS[0] / width, IMAGE_DIMS[1] / height


    temp[(idx*4) + 0] *= width_d
    temp[(idx*4) + 1] *= height_d
    temp[(idx*4) + 2] *= width_d
    temp[(idx*4) + 3] *= height_d
  temp = temp.reshape((4,2))


  for i in range(heatmap.shape[-1]):
    axs[0,i].set_title(targets[i] + " heatmap")
    axs[0,i].imshow(heatmap[0, :,:,i])
    axs[0,i].plot(int(points[i][0].numpy()), int(points[i][1].numpy()), marker="v", color="red")
    axs[1,i].set_title(targets[i] + " original")
    axs[1,i].imshow(frame[0, :,:,:])
    axs[1,i].plot(int(temp[i][0]), int(temp[i][1]), marker="v", color="green")
    axs[1,i].plot(int(points[i][0].numpy()), int(points[i][1].numpy()), marker="v", color="red")
  plt.text(-2300,600,'Euclidean Distance: 4.2390075')
  plt.show()
  print(points)
  print("Euclidean Distance:", np.sqrt(np.linalg.norm(points - temp, ord=2)))
  plt.savefig('data.png', transparent=True)
